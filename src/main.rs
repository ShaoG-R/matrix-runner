//! # Matrix Runner - Configuration-Driven Test Executor
//! # Matrix Runner - é…ç½®é©±åŠ¨çš„æµ‹è¯•æ‰§è¡Œå™¨
//!
//! A powerful, configuration-driven test executor for Rust projects that enables
//! testing across a wide matrix of feature flags and environments. This tool helps
//! ensure comprehensive test coverage by automatically running tests with different
//! feature combinations in isolated environments.
//!
//! ä¸€ä¸ªå¼ºå¤§çš„ã€é…ç½®é©±åŠ¨çš„ Rust é¡¹ç›®æµ‹è¯•æ‰§è¡Œå™¨ï¼Œæ”¯æŒåœ¨å¹¿æ³›çš„ feature æ ‡å¿—å’Œç¯å¢ƒçŸ©é˜µä¸­è¿›è¡Œæµ‹è¯•ã€‚
//! æ­¤å·¥å…·é€šè¿‡åœ¨éš”ç¦»ç¯å¢ƒä¸­è‡ªåŠ¨è¿è¡Œä¸åŒ feature ç»„åˆçš„æµ‹è¯•æ¥å¸®åŠ©ç¡®ä¿å…¨é¢çš„æµ‹è¯•è¦†ç›–ã€‚
//!
//! ## Key Features / ä¸»è¦åŠŸèƒ½
//!
//! - **Matrix Testing**: Run tests across multiple feature flag combinations
//! - **Parallel Execution**: Concurrent test execution with configurable job limits
//! - **Isolated Builds**: Each test runs in its own temporary directory
//! - **HTML Reports**: Generate detailed HTML reports with test results
//! - **Internationalization**: Support for multiple languages (English, Chinese)
//! - **Graceful Shutdown**: Handle interruption signals properly
//!
//! - **çŸ©é˜µæµ‹è¯•**: åœ¨å¤šä¸ª feature æ ‡å¿—ç»„åˆä¸­è¿è¡Œæµ‹è¯•
//! - **å¹¶è¡Œæ‰§è¡Œ**: å…·æœ‰å¯é…ç½®ä½œä¸šé™åˆ¶çš„å¹¶å‘æµ‹è¯•æ‰§è¡Œ
//! - **éš”ç¦»æ„å»º**: æ¯ä¸ªæµ‹è¯•åœ¨è‡ªå·±çš„ä¸´æ—¶ç›®å½•ä¸­è¿è¡Œ
//! - **HTML æŠ¥å‘Š**: ç”ŸæˆåŒ…å«æµ‹è¯•ç»“æœçš„è¯¦ç»† HTML æŠ¥å‘Š
//! - **å›½é™…åŒ–**: æ”¯æŒå¤šç§è¯­è¨€ï¼ˆè‹±è¯­ã€ä¸­æ–‡ï¼‰
//! - **ä¼˜é›…å…³é—­**: æ­£ç¡®å¤„ç†ä¸­æ–­ä¿¡å·

use anyhow::{Context, Result};
use clap::{Parser, Subcommand};
use colored::*;
use futures::{StreamExt, stream};
use runner::i18n::I18nKey;
use serde::Deserialize;
use std::fs;
use std::path::PathBuf;
use tokio::signal;
use tokio_util::sync::CancellationToken;

mod runner;
use runner::config::TestMatrix;
use runner::execution::run_test_case;
use runner::i18n;
use runner::init;
use runner::reporting::{print_summary, print_unexpected_failure_details};

/// Command-line interface definition for the Matrix Runner application.
/// Defines the main CLI structure and available subcommands.
///
/// Matrix Runner åº”ç”¨ç¨‹åºçš„å‘½ä»¤è¡Œç•Œé¢å®šä¹‰ã€‚
/// å®šä¹‰ä¸»è¦çš„ CLI ç»“æ„å’Œå¯ç”¨çš„å­å‘½ä»¤ã€‚
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Cli {
    /// The subcommand to execute / è¦æ‰§è¡Œçš„å­å‘½ä»¤
    #[command(subcommand)]
    command: Commands,
}

/// Available subcommands for the Matrix Runner CLI.
/// Defines the main operations that can be performed with the tool.
///
/// Matrix Runner CLI çš„å¯ç”¨å­å‘½ä»¤ã€‚
/// å®šä¹‰å¯ä»¥ä½¿ç”¨è¯¥å·¥å…·æ‰§è¡Œçš„ä¸»è¦æ“ä½œã€‚
#[derive(Subcommand, Debug)]
enum Commands {
    /// Run tests based on the matrix configuration (default command).
    /// åŸºäºçŸ©é˜µé…ç½®è¿è¡Œæµ‹è¯•ï¼ˆé»˜è®¤å‘½ä»¤ï¼‰ã€‚
    Run(RunArgs),
    /// Launch an interactive wizard to create a new TestMatrix.toml file.
    /// å¯åŠ¨äº¤äº’å¼å‘å¯¼åˆ›å»ºæ–°çš„ TestMatrix.toml æ–‡ä»¶ã€‚
    Init(InitArgs),
}

/// Defines the command-line arguments for the `run` command.
#[derive(Parser, Debug, Default)]
struct RunArgs {
    /// Number of parallel jobs, defaults to (logical CPUs / 2) + 1.
    #[arg(short, long)]
    jobs: Option<usize>,

    /// Path to the test matrix config file, relative to project dir.
    #[arg(short, long, default_value = "TestMatrix.toml")]
    config: PathBuf,

    /// Path to the project directory to test.
    #[arg(long, default_value = ".")]
    project_dir: PathBuf,

    /// Total number of parallel runners for splitting the test matrix.
    #[arg(long)]
    total_runners: Option<usize>,

    /// Index of the current runner (0-based) when splitting the test matrix.
    #[arg(long)]
    runner_index: Option<usize>,

    /// Path to write an HTML report to. If provided, a report will be generated.
    #[arg(long)]
    html: Option<PathBuf>,
}

/// Defines the command-line arguments for the `init` command.
#[derive(Parser, Debug)]
struct InitArgs {
    /// Language for the wizard interface and generated config file.
    /// If not specified, the system language will be auto-detected.
    /// å‘å¯¼ç•Œé¢å’Œç”Ÿæˆçš„é…ç½®æ–‡ä»¶çš„è¯­è¨€ã€‚
    /// å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿè¯­è¨€ã€‚
    #[arg(short, long)]
    language: Option<String>,
}

/// Represents the `[package]` section of a Cargo.toml file.
/// Used to extract the crate name.
/// ä»£è¡¨ Cargo.toml æ–‡ä»¶ä¸­çš„ `[package]` éƒ¨åˆ†ã€‚
/// ç”¨äºæå– crate çš„åç§°ã€‚
#[derive(Deserialize)]
struct Package {
    name: String,
}

/// Represents the top-level structure of a Cargo.toml manifest.
/// ä»£è¡¨ Cargo.toml æ¸…å•çš„é¡¶å±‚ç»“æ„ã€‚
#[derive(Deserialize)]
struct Manifest {
    package: Package,
}

/// Application entry point with async runtime.
/// Initializes the Tokio runtime and handles top-level error reporting.
///
/// å…·æœ‰å¼‚æ­¥è¿è¡Œæ—¶çš„åº”ç”¨ç¨‹åºå…¥å£ç‚¹ã€‚
/// åˆå§‹åŒ– Tokio è¿è¡Œæ—¶å¹¶å¤„ç†é¡¶çº§é”™è¯¯æŠ¥å‘Šã€‚
#[tokio::main]
async fn main() {
    // Run the main application logic and handle any errors
    // è¿è¡Œä¸»åº”ç”¨ç¨‹åºé€»è¾‘å¹¶å¤„ç†ä»»ä½•é”™è¯¯
    if let Err(e) = run_main().await {
        eprintln!("{} {}", "Error:".red(), e);
        std::process::exit(1);
    }
}

async fn run_main() -> Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Run(args) => {
            run_matrix_tests(args).await?;
        }
        Commands::Init(args) => {
            // Determine the language to use: user-specified or auto-detected
            // ç¡®å®šè¦ä½¿ç”¨çš„è¯­è¨€ï¼šç”¨æˆ·æŒ‡å®šçš„æˆ–è‡ªåŠ¨æ£€æµ‹çš„
            let language = args
                .language
                .clone()
                .unwrap_or_else(|| i18n::detect_system_language());

            // Initialize i18n for the init wizard
            // ä¸ºåˆå§‹åŒ–å‘å¯¼åˆå§‹åŒ– i18n
            i18n::init(&language);

            // Show language detection message if it was auto-detected
            // å¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹çš„è¯­è¨€ï¼Œæ˜¾ç¤ºæ£€æµ‹æ¶ˆæ¯
            if args.language.is_none() {
                println!(
                    "ğŸŒ {}",
                    i18n::t_fmt(I18nKey::SystemLanguageDetected, &[&language])
                );
            }

            init::run_init_wizard(&language)?;
        }
    }
    Ok(())
}

async fn run_matrix_tests(args: RunArgs) -> Result<()> {
    let (test_matrix, config_path) = setup_and_parse_config(&args)?;
    i18n::init(&test_matrix.language);

    let (project_root, crate_name) = prepare_environment(&args).await?;

    println!(
        "{}",
        i18n::t_fmt(I18nKey::ProjectRootDetected, &[&project_root.display()])
    );
    println!(
        "{}",
        i18n::t_fmt(I18nKey::TestingCrate, &[&crate_name.yellow()])
    );
    println!(
        "{}",
        i18n::t_fmt(I18nKey::LoadingTestMatrix, &[&config_path.display()])
    );

    let overall_stop_token = setup_signal_handler()?;

    let cases_to_run = filter_and_distribute_cases(test_matrix, &args)?;
    if cases_to_run.is_empty() {
        println!("{}", i18n::t(I18nKey::NoCasesToRun).green());
        std::process::exit(0);
    }

    let (final_results, has_unexpected_failures) = run_tests(
        cases_to_run,
        args.jobs.unwrap_or(num_cpus::get() / 2 + 1),
        &project_root,
        &crate_name,
        overall_stop_token,
    )
    .await?;

    print_summary(&final_results);

    if let Some(report_path) = &args.html {
        println!("\nGenerating HTML report at: {}", report_path.display());
        if let Err(e) = runner::reporting::generate_html_report(&final_results, report_path) {
            eprintln!("{} {}", "Failed to generate HTML report:".red(), e);
        }
    }

    if has_unexpected_failures {
        let unexpected_failures: Vec<_> = final_results
            .iter()
            .filter(|r| r.is_unexpected_failure())
            .collect();
        print_unexpected_failure_details(&unexpected_failures);
        std::process::exit(1);
    } else {
        println!("\n{}", i18n::t(I18nKey::AllTestsPassed).green().bold());
        std::process::exit(0);
    }
}

/// Sets up command-line arguments and loads the test matrix configuration.
/// è®¾ç½®å‘½ä»¤è¡Œå‚æ•°å¹¶åŠ è½½æµ‹è¯•çŸ©é˜µé…ç½®ã€‚
fn setup_and_parse_config(args: &RunArgs) -> Result<(TestMatrix, PathBuf)> {
    let config_path = fs::canonicalize(&args.config)
        .with_context(|| i18n::t_fmt(I18nKey::ConfigReadFailedPath, &[&args.config.display()]))?;

    let config_content = fs::read_to_string(&config_path)
        .with_context(|| i18n::t_fmt(I18nKey::ConfigReadFailedPath, &[&config_path.display()]))?;

    let test_matrix: TestMatrix =
        toml::from_str(&config_content).with_context(|| i18n::t(I18nKey::ConfigParseFailed))?;

    Ok((test_matrix, config_path))
}

/// Prepares the testing environment by fetching dependencies and identifying the crate name.
/// é€šè¿‡é¢„å–ä¾èµ–å’Œè¯†åˆ« crate åç§°æ¥å‡†å¤‡æµ‹è¯•ç¯å¢ƒã€‚
async fn prepare_environment(args: &RunArgs) -> Result<(PathBuf, String)> {
    let project_root = fs::canonicalize(&args.project_dir).with_context(|| {
        i18n::t_fmt(I18nKey::ProjectDirNotFound, &[&args.project_dir.display()])
    })?;

    println!("\n{}", i18n::t(I18nKey::DepFetchStart).cyan());
    let mut fetch_cmd = std::process::Command::new("cargo");
    fetch_cmd.current_dir(&project_root).arg("fetch");

    let fetch_status = fetch_cmd
        .status()
        .context("Failed to execute cargo fetch command")?;
    if !fetch_status.success() {
        return Err(anyhow::anyhow!("{}", i18n::t(I18nKey::CargoFetchFailed)));
    }
    println!("{}", i18n::t(I18nKey::DepFetchSuccess).green());

    let manifest_path = project_root.join("Cargo.toml");
    let manifest_content = fs::read_to_string(&manifest_path)
        .with_context(|| i18n::t_fmt(I18nKey::ManifestReadFailed, &[&manifest_path.display()]))?;

    let manifest: Manifest =
        toml::from_str(&manifest_content).with_context(|| i18n::t(I18nKey::ManifestParseFailed))?;
    let crate_name = manifest.package.name.replace('-', "_");

    Ok((project_root, crate_name))
}

/// Sets up a Ctrl+C signal handler to gracefully shut down the application.
/// è®¾ç½® Ctrl+C ä¿¡å·å¤„ç†å™¨ä»¥ä¼˜é›…åœ°å…³é—­åº”ç”¨ç¨‹åºã€‚
fn setup_signal_handler() -> Result<CancellationToken> {
    let token = CancellationToken::new();
    let signal_token = token.clone();
    tokio::spawn(async move {
        if let Err(e) = signal::ctrl_c().await {
            eprintln!("Failed to listen for Ctrl+C signal: {}", e);
            return;
        }
        println!("\n{}", i18n::t(I18nKey::ShutdownSignal).yellow());
        signal_token.cancel();
    });
    Ok(token)
}

/// Filters test cases based on architecture and distributes them for parallel runners.
/// æ ¹æ®æ¶æ„ç­›é€‰æµ‹è¯•ç”¨ä¾‹å¹¶ä¸ºå¹¶è¡Œè¿è¡Œå™¨åˆ†å‘å®ƒä»¬ã€‚
fn filter_and_distribute_cases(
    test_matrix: TestMatrix,
    args: &RunArgs,
) -> Result<Vec<runner::config::TestCase>> {
    let total_cases_count = test_matrix.cases.len();
    let current_arch = std::env::consts::ARCH;
    println!(
        "{}",
        i18n::t_fmt(I18nKey::CurrentArch, &[&current_arch.yellow()])
    );

    let all_cases: Vec<_> = test_matrix
        .cases
        .into_iter()
        .filter(|case| case.arch.is_empty() || case.arch.iter().any(|a| a == current_arch))
        .collect();

    let filtered_count = total_cases_count - all_cases.len();
    if filtered_count > 0 {
        println!(
            "{}",
            i18n::t_fmt(
                I18nKey::FilteredArchCases,
                &[&filtered_count, &all_cases.len()]
            )
            .yellow()
        );
    }

    match (args.total_runners, args.runner_index) {
        (Some(total), Some(index)) => {
            if index >= total {
                return Err(anyhow::anyhow!("{}", i18n::t(I18nKey::RunnerIndexInvalid)));
            }
            let cases_for_this_runner = all_cases
                .into_iter()
                .enumerate()
                .filter_map(|(i, case)| if i % total == index { Some(case) } else { None })
                .collect::<Vec<_>>();
            println!(
                "{}",
                i18n::t_fmt(
                    I18nKey::RunningAsSplitRunner,
                    &[&(index + 1), &total, &cases_for_this_runner.len()]
                )
                .yellow()
            );
            Ok(cases_for_this_runner)
        }
        (None, None) => {
            println!("{}", i18n::t(I18nKey::RunningAsSingleRunner).yellow());
            Ok(all_cases)
        }
        _ => Err(anyhow::anyhow!(
            "{}",
            i18n::t(I18nKey::RunnerFlagsInconsistent)
        )),
    }
}

/// Runs the selected test cases and reports the summary.
/// è¿è¡Œé€‰å®šçš„æµ‹è¯•ç”¨ä¾‹å¹¶æŠ¥å‘Šæ‘˜è¦ã€‚
async fn run_tests(
    cases_to_run: Vec<runner::config::TestCase>,
    jobs: usize,
    project_root: &PathBuf,
    crate_name: &str,
    overall_stop_token: CancellationToken,
) -> Result<(
    Vec<runner::models::TestResult>, // final_results
    bool,                            // has_unexpected_failures
)> {
    println!("{}", i18n::t(I18nKey::TempDirCleanupInfo).green());
    println!("{}", i18n::t(I18nKey::FailureArtifactInfo).yellow());

    let current_os = std::env::consts::OS;
    println!(
        "{}",
        i18n::t_fmt(I18nKey::CurrentOs, &[&current_os.yellow()])
    );

    let (flaky_cases, safe_cases): (Vec<_>, Vec<_>) = cases_to_run
        .into_iter()
        .partition(|case| case.allow_failure.iter().any(|s| s == current_os));

    let safe_cases_count = safe_cases.len();
    let flaky_cases_count = flaky_cases.len();
    if flaky_cases_count > 0 {
        println!(
            "{}",
            i18n::t_fmt(I18nKey::FlakyCasesFound, &[&flaky_cases_count]).yellow()
        );
    }

    let mut results = Vec::new();
    let fast_fail_token = CancellationToken::new();

    println!(
        "\n{}",
        i18n::t_fmt(I18nKey::RunningSafeCases, &[&safe_cases_count, &jobs]).green()
    );
    let safe_stream = stream::iter(safe_cases.into_iter().map(|case| {
        let case_stop_token = fast_fail_token.clone();
        let global_stop_token = overall_stop_token.clone();
        let root = project_root.clone();
        let name = crate_name.to_string();
        tokio::spawn(async move {
            tokio::select! {
                res = run_test_case(case, &root, &name) => {
                    res.unwrap_or_else(|e| {
                            eprintln!("Task failed: {e:?}");
                            runner::models::TestResult::Skipped
                        })
                },
                _ = case_stop_token.cancelled() => runner::models::TestResult::Skipped,
                _ = global_stop_token.cancelled() => runner::models::TestResult::Skipped,
            }
        })
    }));

    let mut safe_processed_stream = safe_stream.buffer_unordered(jobs);
    while let Some(result) = safe_processed_stream.next().await {
        let test_result = result.unwrap_or_else(|e| {
            eprintln!("Task join error: {e:?}");
            runner::models::TestResult::Skipped
        });
        if test_result.is_unexpected_failure() && !fast_fail_token.is_cancelled() {
            println!("\n{}", i18n::t(I18nKey::FastFailTriggered).red().bold());
            fast_fail_token.cancel();
        }
        results.push(test_result);
    }

    if !overall_stop_token.is_cancelled() && flaky_cases_count > 0 {
        println!(
            "\n{}",
            i18n::t_fmt(I18nKey::RunningFlakyCases, &[&flaky_cases_count, &jobs]).green()
        );
        let flaky_stream = stream::iter(flaky_cases.into_iter().map(|case| {
            let global_stop_token = overall_stop_token.clone();
            let root = project_root.clone();
            let name = crate_name.to_string();
            tokio::spawn(async move {
                tokio::select! {
                    res = run_test_case(case, &root, &name) => {
                        res.unwrap_or_else(|e| {
                                eprintln!("Task failed: {e:?}");
                                runner::models::TestResult::Skipped
                            })
                    },
                    _ = global_stop_token.cancelled() => runner::models::TestResult::Skipped,
                }
            })
        }));

        let mut flaky_processed_stream = flaky_stream.buffer_unordered(jobs);
        while let Some(result) = flaky_processed_stream.next().await {
            let test_result = result.unwrap_or_else(|e| {
                eprintln!("Task join error: {e:?}");
                runner::models::TestResult::Skipped
            });
            results.push(test_result);
        }
    }

    let has_unexpected_failures = results.iter().any(|r| r.is_unexpected_failure());
    Ok((results, has_unexpected_failures))
}
